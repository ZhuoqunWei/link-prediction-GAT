{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.__version__)\n",
        "print(torch.version.cuda)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZxm234VUZsd",
        "outputId": "ee958859-b155-4ae5-cf64-329f89605633"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.5.1+cu121\n",
            "12.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JKTxhnpQTyQ-",
        "outputId": "f286bffd-0070-4fcf-e315-25d41d54a360"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ogb in /usr/local/lib/python3.10/dist-packages (1.3.6)\n",
            "Requirement already satisfied: torch-geometric in /usr/local/lib/python3.10/dist-packages (2.6.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (2.5.1+cu121)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.26.4)\n",
            "Requirement already satisfied: tqdm>=4.29.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (4.66.6)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.5.2)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (2.2.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.16.0)\n",
            "Requirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (2.2.3)\n",
            "Requirement already satisfied: outdated>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (0.2.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.11.9)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2024.10.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.4)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.2.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.32.3)\n",
            "Requirement already satisfied: setuptools>=44 in /usr/local/lib/python3.10/dist-packages (from outdated>=0.2.0->ogb) (75.1.0)\n",
            "Requirement already satisfied: littleutils in /usr/local/lib/python3.10/dist-packages (from outdated>=0.2.0->ogb) (0.2.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->ogb) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->ogb) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->ogb) (2024.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->ogb) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->ogb) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->ogb) (3.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (3.4.2)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.6.0->ogb) (1.3.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.18.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2024.8.30)\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.5.1+cu121.html\n",
            "Requirement already satisfied: torch-scatter in /usr/local/lib/python3.10/dist-packages (2.1.2+pt25cu121)\n",
            "Requirement already satisfied: torch-sparse in /usr/local/lib/python3.10/dist-packages (0.6.18+pt25cu121)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-sparse) (1.13.1)\n",
            "Requirement already satisfied: numpy<2.3,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from scipy->torch-sparse) (1.26.4)\n"
          ]
        }
      ],
      "source": [
        "# Install necessary packages\n",
        "!pip install ogb torch-geometric\n",
        "!pip install torch-scatter torch-sparse -f https://data.pyg.org/whl/torch-2.5.1+cu121.html\n",
        "\n",
        "import argparse\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from torch_sparse import SparseTensor\n",
        "import torch_geometric.transforms as T\n",
        "from torch_geometric.nn import GATConv\n",
        "from ogb.linkproppred import PygLinkPropPredDataset, Evaluator"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Logger(object):\n",
        "    def __init__(self, runs, args):\n",
        "        self.runs = runs\n",
        "        self.results = [[] for _ in range(runs)]\n",
        "        self.args = args\n",
        "\n",
        "    def add_result(self, run, result):\n",
        "        assert len(result) == 3\n",
        "        self.results[run].append(result)\n",
        "\n",
        "    def print_statistics(self, run=None):\n",
        "        if run is not None:\n",
        "            result = 100 * torch.tensor(self.results[run])\n",
        "            print(f'Run {run + 1:02d}:')\n",
        "            print(f'  Best valid: {result[:,1].max():.2f}')\n",
        "            idx = result[:,1].argmax()\n",
        "            print(f'  Test at best valid: {result[idx,2]:.2f}')\n",
        "        else:\n",
        "            result = 100 * torch.tensor(self.results)\n",
        "            best_results = []\n",
        "            for r in result:\n",
        "                valid = r[:,1]\n",
        "                test = r[:,2]\n",
        "                best_val = valid.max().item()\n",
        "                best_test = test[valid.argmax()].item()\n",
        "                best_results.append((best_val, best_test))\n",
        "            best_val = torch.tensor(best_results)[:,0]\n",
        "            best_test = torch.tensor(best_results)[:,1]\n",
        "            print(f'All runs:')\n",
        "            print(f'  Valid: {best_val.mean():.2f} ± {best_val.std():.2f}')\n",
        "            print(f'  Test: {best_test.mean():.2f} ± {best_test.std():.2f}')"
      ],
      "metadata": {
        "id": "3PczMJmmT4oe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#########################################\n",
        "# Model Definitions\n",
        "#########################################\n",
        "# After loading and processing the data:\n",
        "data.adj_t = data.adj_t.set_diag()  # Add self-loops beforehand\n",
        "\n",
        "class GAT(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers,\n",
        "                 dropout, heads=1):\n",
        "        super(GAT, self).__init__()\n",
        "\n",
        "        # We assume edge weights are scalar, so edge_dim=1\n",
        "        # and set add_self_loops=False since we've already added them.\n",
        "        self.convs = torch.nn.ModuleList()\n",
        "        self.convs.append(GATConv(in_channels, hidden_channels, heads=heads,\n",
        "                                  dropout=dropout, edge_dim=1, add_self_loops=False))\n",
        "        for _ in range(num_layers - 2):\n",
        "            self.convs.append(GATConv(hidden_channels * heads, hidden_channels,\n",
        "                                      heads=heads, dropout=dropout, edge_dim=1, add_self_loops=False))\n",
        "        self.convs.append(GATConv(hidden_channels * heads, out_channels,\n",
        "                                  heads=heads, dropout=dropout, edge_dim=1, add_self_loops=False))\n",
        "\n",
        "        self.dropout = dropout\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for conv in self.convs:\n",
        "            conv.reset_parameters()\n",
        "\n",
        "    def forward(self, x, adj_t):\n",
        "        # Extract edge weights from the sparse adjacency\n",
        "        edge_weight = adj_t.storage.value()\n",
        "\n",
        "        for conv in self.convs[:-1]:\n",
        "            x = conv(x, adj_t, edge_attr=edge_weight)\n",
        "            x = F.elu(x)\n",
        "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = self.convs[-1](x, adj_t, edge_attr=edge_weight)\n",
        "        return x\n",
        "\n",
        "# The rest of your training and testing code remains the same.\n",
        "\n",
        "\n",
        "\n",
        "class LinkPredictor(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers,\n",
        "                 dropout):\n",
        "        super(LinkPredictor, self).__init__()\n",
        "\n",
        "        self.lins = torch.nn.ModuleList()\n",
        "        self.lins.append(torch.nn.Linear(in_channels, hidden_channels))\n",
        "        for _ in range(num_layers - 2):\n",
        "            self.lins.append(torch.nn.Linear(hidden_channels, hidden_channels))\n",
        "        self.lins.append(torch.nn.Linear(hidden_channels, out_channels))\n",
        "\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for lin in self.lins:\n",
        "            lin.reset_parameters()\n",
        "\n",
        "    def forward(self, x_i, x_j):\n",
        "        x = x_i * x_j\n",
        "        for lin in self.lins[:-1]:\n",
        "            x = lin(x)\n",
        "            x = F.relu(x)\n",
        "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = self.lins[-1](x)\n",
        "        return torch.sigmoid(x)"
      ],
      "metadata": {
        "id": "REqj5ErGWK_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#########################################\n",
        "# Training and Testing Functions\n",
        "#########################################\n",
        "def train(model, predictor, data, split_edge, optimizer, batch_size):\n",
        "    model.train()\n",
        "    predictor.train()\n",
        "\n",
        "    pos_train_edge = split_edge['train']['edge'].to(data.x.device)\n",
        "\n",
        "    total_loss = total_examples = 0\n",
        "    for perm in DataLoader(range(pos_train_edge.size(0)), batch_size,\n",
        "                           shuffle=True):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        h = model(data.x, data.adj_t)\n",
        "\n",
        "        edge = pos_train_edge[perm].t()\n",
        "\n",
        "        pos_out = predictor(h[edge[0]], h[edge[1]])\n",
        "        pos_loss = -torch.log(pos_out + 1e-15).mean()\n",
        "\n",
        "        # Negative sampling: random edges\n",
        "        edge = torch.randint(0, data.num_nodes, edge.size(), dtype=torch.long,\n",
        "                             device=h.device)\n",
        "        neg_out = predictor(h[edge[0]], h[edge[1]])\n",
        "        neg_loss = -torch.log(1 - neg_out + 1e-15).mean()\n",
        "\n",
        "        loss = pos_loss + neg_loss\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        torch.nn.utils.clip_grad_norm_(predictor.parameters(), 1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        num_examples = pos_out.size(0)\n",
        "        total_loss += loss.item() * num_examples\n",
        "        total_examples += num_examples\n",
        "\n",
        "    return total_loss / total_examples\n",
        "\n",
        "@torch.no_grad()\n",
        "def test(model, predictor, data, split_edge, evaluator, batch_size):\n",
        "    model.eval()\n",
        "    predictor.eval()\n",
        "\n",
        "    h = model(data.x, data.adj_t)\n",
        "\n",
        "    pos_train_edge = split_edge['train']['edge'].to(h.device)\n",
        "    pos_valid_edge = split_edge['valid']['edge'].to(h.device)\n",
        "    neg_valid_edge = split_edge['valid']['edge_neg'].to(h.device)\n",
        "    pos_test_edge = split_edge['test']['edge'].to(h.device)\n",
        "    neg_test_edge = split_edge['test']['edge_neg'].to(h.device)\n",
        "\n",
        "    pos_train_preds = []\n",
        "    for perm in DataLoader(range(pos_train_edge.size(0)), batch_size):\n",
        "        edge = pos_train_edge[perm].t()\n",
        "        pos_train_preds += [predictor(h[edge[0]], h[edge[1]]).squeeze().cpu()]\n",
        "    pos_train_pred = torch.cat(pos_train_preds, dim=0)\n",
        "\n",
        "    pos_valid_preds = []\n",
        "    for perm in DataLoader(range(pos_valid_edge.size(0)), batch_size):\n",
        "        edge = pos_valid_edge[perm].t()\n",
        "        pos_valid_preds += [predictor(h[edge[0]], h[edge[1]]).squeeze().cpu()]\n",
        "    pos_valid_pred = torch.cat(pos_valid_preds, dim=0)\n",
        "\n",
        "    neg_valid_preds = []\n",
        "    for perm in DataLoader(range(neg_valid_edge.size(0)), batch_size):\n",
        "        edge = neg_valid_edge[perm].t()\n",
        "        neg_valid_preds += [predictor(h[edge[0]], h[edge[1]]).squeeze().cpu()]\n",
        "    neg_valid_pred = torch.cat(neg_valid_preds, dim=0)\n",
        "\n",
        "    h = model(data.x, data.full_adj_t)\n",
        "\n",
        "    pos_test_preds = []\n",
        "    for perm in DataLoader(range(pos_test_edge.size(0)), batch_size):\n",
        "        edge = pos_test_edge[perm].t()\n",
        "        pos_test_preds += [predictor(h[edge[0]], h[edge[1]]).squeeze().cpu()]\n",
        "    pos_test_pred = torch.cat(pos_test_preds, dim=0)\n",
        "\n",
        "    neg_test_preds = []\n",
        "    for perm in DataLoader(range(neg_test_edge.size(0)), batch_size):\n",
        "        edge = neg_test_edge[perm].t()\n",
        "        neg_test_preds += [predictor(h[edge[0]], h[edge[1]]).squeeze().cpu()]\n",
        "    neg_test_pred = torch.cat(neg_test_preds, dim=0)\n",
        "\n",
        "    results = {}\n",
        "    for K in [10, 50, 100]:\n",
        "        evaluator.K = K\n",
        "        train_hits = evaluator.eval({\n",
        "            'y_pred_pos': pos_train_pred,\n",
        "            'y_pred_neg': neg_valid_pred,\n",
        "        })[f'hits@{K}']\n",
        "        valid_hits = evaluator.eval({\n",
        "            'y_pred_pos': pos_valid_pred,\n",
        "            'y_pred_neg': neg_valid_pred,\n",
        "        })[f'hits@{K}']\n",
        "        test_hits = evaluator.eval({\n",
        "            'y_pred_pos': pos_test_pred,\n",
        "            'y_pred_neg': neg_test_pred,\n",
        "        })[f'hits@{K}']\n",
        "\n",
        "        results[f'Hits@{K}'] = (train_hits, valid_hits, test_hits)\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "9sBmE6NYWaMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#########################################\n",
        "# Main Execution\n",
        "#########################################\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser(description='OGBL-COLLAB (GAT)')\n",
        "    parser.add_argument('--device', type=int, default=0)\n",
        "    parser.add_argument('--log_steps', type=int, default=1)\n",
        "    parser.add_argument('--num_layers', type=int, default=4)\n",
        "    parser.add_argument('--hidden_channels', type=int, default=256)\n",
        "    parser.add_argument('--dropout', type=float, default=0.1)\n",
        "    parser.add_argument('--batch_size', type=int, default=32 * 1024)\n",
        "    parser.add_argument('--lr', type=float, default=0.001)\n",
        "    parser.add_argument('--epochs', type=int, default=400)\n",
        "    parser.add_argument('--eval_steps', type=int, default=10)\n",
        "    parser.add_argument('--runs', type=int, default=1)\n",
        "    parser.add_argument('--heads', type=int, default=1) # Example of multi-head attention\n",
        "    parser.add_argument('--use_valedges_as_input', action='store_true')\n",
        "    args = parser.parse_args([])  # For colab, manually pass no args.\n",
        "\n",
        "    device = f'cuda:{args.device}' if torch.cuda.is_available() else 'cpu'\n",
        "    device = torch.device(device)\n",
        "\n",
        "    dataset = PygLinkPropPredDataset(name='ogbl-collab')\n",
        "    data = dataset[0]\n",
        "    edge_index = data.edge_index\n",
        "    data.edge_weight = data.edge_weight.view(-1).to(torch.float)\n",
        "    data = T.ToSparseTensor()(data)\n",
        "\n",
        "    split_edge = dataset.get_edge_split()\n",
        "\n",
        "    # Use training + validation edges for inference on test set if needed\n",
        "    if args.use_valedges_as_input:\n",
        "        val_edge_index = split_edge['valid']['edge'].t()\n",
        "        full_edge_index = torch.cat([edge_index, val_edge_index], dim=-1)\n",
        "        data.full_adj_t = SparseTensor.from_edge_index(full_edge_index).t()\n",
        "        data.full_adj_t = data.full_adj_t.to_symmetric()\n",
        "    else:\n",
        "        data.full_adj_t = data.adj_t\n",
        "\n",
        "    data = data.to(device)\n",
        "\n",
        "    # Instantiate GAT model\n",
        "    model = GAT(data.num_features, args.hidden_channels,\n",
        "                args.hidden_channels, args.num_layers,\n",
        "                args.dropout, heads=args.heads).to(device)\n",
        "\n",
        "    predictor = LinkPredictor(args.hidden_channels, args.hidden_channels, 1,\n",
        "                              args.num_layers, args.dropout).to(device)\n",
        "\n",
        "    evaluator = Evaluator(name='ogbl-collab')\n",
        "    loggers = {\n",
        "        'Hits@10': Logger(args.runs, args),\n",
        "        'Hits@50': Logger(args.runs, args),\n",
        "        'Hits@100': Logger(args.runs, args),\n",
        "    }\n",
        "\n",
        "    for run in range(args.runs):\n",
        "        model.reset_parameters()\n",
        "        predictor.reset_parameters()\n",
        "        optimizer = torch.optim.Adam(\n",
        "            list(model.parameters()) + list(predictor.parameters()),\n",
        "            lr=args.lr)\n",
        "\n",
        "        for epoch in range(1, 1 + args.epochs):\n",
        "            loss = train(model, predictor, data, split_edge, optimizer,\n",
        "                         args.batch_size)\n",
        "\n",
        "            if epoch % args.eval_steps == 0:\n",
        "                results = test(model, predictor, data, split_edge, evaluator,\n",
        "                               args.batch_size)\n",
        "                for key, result in results.items():\n",
        "                    loggers[key].add_result(run, result)\n",
        "\n",
        "                if epoch % args.log_steps == 0:\n",
        "                    for key, result in results.items():\n",
        "                        train_hits, valid_hits, test_hits = result\n",
        "                        print(key)\n",
        "                        print(f'Run: {run + 1:02d}, '\n",
        "                              f'Epoch: {epoch:02d}, '\n",
        "                              f'Loss: {loss:.4f}, '\n",
        "                              f'Train: {100 * train_hits:.2f}%, '\n",
        "                              f'Valid: {100 * valid_hits:.2f}%, '\n",
        "                              f'Test: {100 * test_hits:.2f}%')\n",
        "                    print('---')\n",
        "\n",
        "        for key in loggers.keys():\n",
        "            print(key)\n",
        "            loggers[key].print_statistics(run)\n",
        "\n",
        "    for key in loggers.keys():\n",
        "        print(key)\n",
        "        loggers[key].print_statistics()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 515
        },
        "id": "HNDCJlndWf-g",
        "outputId": "2d20be86-2cf6-473c-894b-24eed1132e81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ogb/linkproppred/dataset_pyg.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  self.data, self.slices = torch.load(self.processed_paths[0])\n",
            "/usr/local/lib/python3.10/dist-packages/ogb/linkproppred/dataset_pyg.py:77: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  train = replace_numpy_with_torchtensor(torch.load(osp.join(path, 'train.pt')))\n",
            "/usr/local/lib/python3.10/dist-packages/ogb/linkproppred/dataset_pyg.py:78: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  valid = replace_numpy_with_torchtensor(torch.load(osp.join(path, 'valid.pt')))\n",
            "/usr/local/lib/python3.10/dist-packages/ogb/linkproppred/dataset_pyg.py:79: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  test = replace_numpy_with_torchtensor(torch.load(osp.join(path, 'test.pt')))\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NotImplementedError",
          "evalue": "The usage of 'edge_attr' and 'add_self_loops' simultaneously is currently not yet supported for 'edge_index' in a 'SparseTensor' form",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-45dc42fc0cc1>\u001b[0m in \u001b[0;36m<cell line: 94>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-13-45dc42fc0cc1>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             loss = train(model, predictor, data, split_edge, optimizer,\n\u001b[0m\u001b[1;32m     66\u001b[0m                          args.batch_size)\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-321313578659>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, predictor, data, split_edge, optimizer, batch_size)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madj_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0medge\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpos_train_edge\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mperm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-891dc73f3abf>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, adj_t)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mconv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madj_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_attr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0medge_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch_geometric/nn/conv/gat_conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, edge_index, edge_attr, size, return_attention_weights)\u001b[0m\n\u001b[1;32m    354\u001b[0m                     \u001b[0medge_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch_sparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_diag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m                     raise NotImplementedError(\n\u001b[0m\u001b[1;32m    357\u001b[0m                         \u001b[0;34m\"The usage of 'edge_attr' and 'add_self_loops' \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m                         \u001b[0;34m\"simultaneously is currently not yet supported for \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: The usage of 'edge_attr' and 'add_self_loops' simultaneously is currently not yet supported for 'edge_index' in a 'SparseTensor' form"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"Your Modified Code\"\"\"\n",
        "\n",
        "import torch\n",
        "print(torch.__version__)\n",
        "print(torch.version.cuda)\n",
        "\n",
        "!pip install ogb torch-geometric\n",
        "!pip install torch-scatter torch-sparse -f https://data.pyg.org/whl/torch-2.5.1+cu121.html\n",
        "\n",
        "import argparse\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from torch_sparse import SparseTensor\n",
        "import torch_geometric.transforms as T\n",
        "from torch_geometric.nn import GATConv\n",
        "from ogb.linkproppred import PygLinkPropPredDataset, Evaluator\n",
        "\n",
        "class Logger(object):\n",
        "    def __init__(self, runs, args):\n",
        "        self.runs = runs\n",
        "        self.results = [[] for _ in range(runs)]\n",
        "        self.args = args\n",
        "\n",
        "    def add_result(self, run, result):\n",
        "        assert len(result) == 3\n",
        "        self.results[run].append(result)\n",
        "\n",
        "    def print_statistics(self, run=None):\n",
        "        if run is not None:\n",
        "            result = 100 * torch.tensor(self.results[run])\n",
        "            print(f'Run {run + 1:02d}:')\n",
        "            print(f'  Best valid: {result[:,1].max():.2f}')\n",
        "            idx = result[:,1].argmax()\n",
        "            print(f'  Test at best valid: {result[idx,2]:.2f}')\n",
        "        else:\n",
        "            result = 100 * torch.tensor(self.results)\n",
        "            best_results = []\n",
        "            for r in result:\n",
        "                valid = r[:,1]\n",
        "                test = r[:,2]\n",
        "                best_val = valid.max().item()\n",
        "                best_test = test[valid.argmax()].item()\n",
        "                best_results.append((best_val, best_test))\n",
        "            best_val = torch.tensor(best_results)[:,0]\n",
        "            best_test = torch.tensor(best_results)[:,1]\n",
        "            print(f'All runs:')\n",
        "            print(f'  Valid: {best_val.mean():.2f} ± {best_val.std():.2f}')\n",
        "            print(f'  Test: {best_test.mean():.2f} ± {best_test.std():.2f}')\n",
        "\n",
        "#########################################\n",
        "# Model Definitions\n",
        "#########################################\n",
        "class GAT(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers,\n",
        "                 dropout, heads=1):\n",
        "        super(GAT, self).__init__()\n",
        "\n",
        "        # Using edge_dim=1 to support edge attributes,\n",
        "        # and add_self_loops=False since we will manually add them.\n",
        "        self.convs = torch.nn.ModuleList()\n",
        "        self.convs.append(GATConv(in_channels, hidden_channels, heads=heads, dropout=dropout, edge_dim=1, add_self_loops=False))\n",
        "        for _ in range(num_layers - 2):\n",
        "            self.convs.append(GATConv(hidden_channels * heads, hidden_channels, heads=heads, dropout=dropout, edge_dim=1, add_self_loops=False))\n",
        "        self.convs.append(GATConv(hidden_channels * heads, out_channels, heads=heads, dropout=dropout, edge_dim=1, add_self_loops=False))\n",
        "\n",
        "        self.dropout = dropout\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for conv in self.convs:\n",
        "            conv.reset_parameters()\n",
        "\n",
        "    def forward(self, x, adj_t):\n",
        "        # Extract edge weights from the sparse adjacency\n",
        "        edge_weight = adj_t.storage.value()\n",
        "\n",
        "        for conv in self.convs[:-1]:\n",
        "            x = conv(x, adj_t, edge_attr=edge_weight)\n",
        "            x = F.elu(x)\n",
        "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = self.convs[-1](x, adj_t, edge_attr=edge_weight)\n",
        "        return x\n",
        "\n",
        "class LinkPredictor(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers,\n",
        "                 dropout):\n",
        "        super(LinkPredictor, self).__init__()\n",
        "\n",
        "        self.lins = torch.nn.ModuleList()\n",
        "        self.lins.append(torch.nn.Linear(in_channels, hidden_channels))\n",
        "        for _ in range(num_layers - 2):\n",
        "            self.lins.append(torch.nn.Linear(hidden_channels, hidden_channels))\n",
        "        self.lins.append(torch.nn.Linear(hidden_channels, out_channels))\n",
        "\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for lin in self.lins:\n",
        "            lin.reset_parameters()\n",
        "\n",
        "    def forward(self, x_i, x_j):\n",
        "        x = x_i * x_j\n",
        "        for lin in self.lins[:-1]:\n",
        "            x = lin(x)\n",
        "            x = F.relu(x)\n",
        "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = self.lins[-1](x)\n",
        "        return torch.sigmoid(x)\n",
        "\n",
        "#########################################\n",
        "# Training and Testing Functions\n",
        "#########################################\n",
        "def train(model, predictor, data, split_edge, optimizer, batch_size):\n",
        "    model.train()\n",
        "    predictor.train()\n",
        "\n",
        "    pos_train_edge = split_edge['train']['edge'].to(data.x.device)\n",
        "\n",
        "    total_loss = total_examples = 0\n",
        "    for perm in DataLoader(range(pos_train_edge.size(0)), batch_size,\n",
        "                           shuffle=True):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        h = model(data.x, data.adj_t)\n",
        "\n",
        "        edge = pos_train_edge[perm].t()\n",
        "\n",
        "        pos_out = predictor(h[edge[0]], h[edge[1]])\n",
        "        pos_loss = -torch.log(pos_out + 1e-15).mean()\n",
        "\n",
        "        # Negative sampling: random edges\n",
        "        edge = torch.randint(0, data.num_nodes, edge.size(), dtype=torch.long,\n",
        "                             device=h.device)\n",
        "        neg_out = predictor(h[edge[0]], h[edge[1]])\n",
        "        neg_loss = -torch.log(1 - neg_out + 1e-15).mean()\n",
        "\n",
        "        loss = pos_loss + neg_loss\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        torch.nn.utils.clip_grad_norm_(predictor.parameters(), 1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        num_examples = pos_out.size(0)\n",
        "        total_loss += loss.item() * num_examples\n",
        "        total_examples += num_examples\n",
        "\n",
        "    return total_loss / total_examples\n",
        "\n",
        "@torch.no_grad()\n",
        "def test(model, predictor, data, split_edge, evaluator, batch_size):\n",
        "    model.eval()\n",
        "    predictor.eval()\n",
        "\n",
        "    h = model(data.x, data.adj_t)\n",
        "\n",
        "    pos_train_edge = split_edge['train']['edge'].to(h.device)\n",
        "    pos_valid_edge = split_edge['valid']['edge'].to(h.device)\n",
        "    neg_valid_edge = split_edge['valid']['edge_neg'].to(h.device)\n",
        "    pos_test_edge = split_edge['test']['edge'].to(h.device)\n",
        "    neg_test_edge = split_edge['test']['edge_neg'].to(h.device)\n",
        "\n",
        "    pos_train_preds = []\n",
        "    for perm in DataLoader(range(pos_train_edge.size(0)), batch_size):\n",
        "        edge = pos_train_edge[perm].t()\n",
        "        pos_train_preds += [predictor(h[edge[0]], h[edge[1]]).squeeze().cpu()]\n",
        "    pos_train_pred = torch.cat(pos_train_preds, dim=0)\n",
        "\n",
        "    pos_valid_preds = []\n",
        "    for perm in DataLoader(range(pos_valid_edge.size(0)), batch_size):\n",
        "        edge = pos_valid_edge[perm].t()\n",
        "        pos_valid_preds += [predictor(h[edge[0]], h[edge[1]]).squeeze().cpu()]\n",
        "    pos_valid_pred = torch.cat(pos_valid_preds, dim=0)\n",
        "\n",
        "    neg_valid_preds = []\n",
        "    for perm in DataLoader(range(neg_valid_edge.size(0)), batch_size):\n",
        "        edge = neg_valid_edge[perm].t()\n",
        "        neg_valid_preds += [predictor(h[edge[0]], h[edge[1]]).squeeze().cpu()]\n",
        "    neg_valid_pred = torch.cat(neg_valid_preds, dim=0)\n",
        "\n",
        "    # Use the full adjacency including validation edges for final test inference if desired\n",
        "    h = model(data.x, data.full_adj_t)\n",
        "\n",
        "    pos_test_preds = []\n",
        "    for perm in DataLoader(range(pos_test_edge.size(0)), batch_size):\n",
        "        edge = pos_test_edge[perm].t()\n",
        "        pos_test_preds += [predictor(h[edge[0]], h[edge[1]]).squeeze().cpu()]\n",
        "    pos_test_pred = torch.cat(pos_test_preds, dim=0)\n",
        "\n",
        "    neg_test_preds = []\n",
        "    for perm in DataLoader(range(neg_test_edge.size(0)), batch_size):\n",
        "        edge = neg_test_edge[perm].t()\n",
        "        neg_test_preds += [predictor(h[edge[0]], h[edge[1]]).squeeze().cpu()]\n",
        "    neg_test_pred = torch.cat(neg_test_preds, dim=0)\n",
        "\n",
        "    results = {}\n",
        "    for K in [10, 50, 100]:\n",
        "        evaluator.K = K\n",
        "        train_hits = evaluator.eval({\n",
        "            'y_pred_pos': pos_train_pred,\n",
        "            'y_pred_neg': neg_valid_pred,\n",
        "        })[f'hits@{K}']\n",
        "        valid_hits = evaluator.eval({\n",
        "            'y_pred_pos': pos_valid_pred,\n",
        "            'y_pred_neg': neg_valid_pred,\n",
        "        })[f'hits@{K}']\n",
        "        test_hits = evaluator.eval({\n",
        "            'y_pred_pos': pos_test_pred,\n",
        "            'y_pred_neg': neg_test_pred,\n",
        "        })[f'hits@{K}']\n",
        "\n",
        "        results[f'Hits@{K}'] = (train_hits, valid_hits, test_hits)\n",
        "\n",
        "    return results\n",
        "\n",
        "#########################################\n",
        "# Main Execution\n",
        "#########################################\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser(description='OGBL-COLLAB (GAT)')\n",
        "    parser.add_argument('--device', type=int, default=0)\n",
        "    parser.add_argument('--log_steps', type=int, default=1)\n",
        "    parser.add_argument('--num_layers', type=int, default=4)\n",
        "    parser.add_argument('--hidden_channels', type=int, default=256)\n",
        "    parser.add_argument('--dropout', type=float, default=0.1)\n",
        "    parser.add_argument('--batch_size', type=int, default=32 * 1024)\n",
        "    parser.add_argument('--lr', type=float, default=0.001)\n",
        "    parser.add_argument('--epochs', type=int, default=400)\n",
        "    parser.add_argument('--eval_steps', type=int, default=10)\n",
        "    parser.add_argument('--runs', type=int, default=1)\n",
        "    parser.add_argument('--heads', type=int, default=1)\n",
        "    parser.add_argument('--use_valedges_as_input', action='store_true')\n",
        "    args = parser.parse_args([])  # For colab, manually pass no args.\n",
        "\n",
        "    device = f'cuda:{args.device}' if torch.cuda.is_available() else 'cpu'\n",
        "    device = torch.device(device)\n",
        "\n",
        "    dataset = PygLinkPropPredDataset(name='ogbl-collab')\n",
        "    data = dataset[0]\n",
        "    # data.edge_weight is assumed to be provided by the dataset\n",
        "    data.edge_weight = data.edge_weight.view(-1).to(torch.float)\n",
        "    data = T.ToSparseTensor()(data)\n",
        "\n",
        "    # Add self-loops\n",
        "    data.adj_t = data.adj_t.set_diag()\n",
        "\n",
        "    split_edge = dataset.get_edge_split()\n",
        "\n",
        "    # Use training + validation edges for inference on test set if needed\n",
        "    if args.use_valedges_as_input:\n",
        "        val_edge_index = split_edge['valid']['edge'].t()\n",
        "        full_edge_index = torch.cat([data.edge_index, val_edge_index], dim=-1)\n",
        "        data.full_adj_t = SparseTensor.from_edge_index(full_edge_index).t()\n",
        "        data.full_adj_t = data.full_adj_t.to_symmetric()\n",
        "    else:\n",
        "        data.full_adj_t = data.adj_t\n",
        "\n",
        "    data = data.to(device)\n",
        "\n",
        "    # Instantiate GAT model with edge_dim=1 and no self-loops added by the layer\n",
        "    model = GAT(data.num_features, args.hidden_channels,\n",
        "                args.hidden_channels, args.num_layers,\n",
        "                args.dropout, heads=args.heads).to(device)\n",
        "\n",
        "    predictor = LinkPredictor(args.hidden_channels, args.hidden_channels, 1,\n",
        "                              args.num_layers, args.dropout).to(device)\n",
        "\n",
        "    evaluator = Evaluator(name='ogbl-collab')\n",
        "    loggers = {\n",
        "        'Hits@10': Logger(args.runs, args),\n",
        "        'Hits@50': Logger(args.runs, args),\n",
        "        'Hits@100': Logger(args.runs, args),\n",
        "    }\n",
        "\n",
        "    for run in range(args.runs):\n",
        "        model.reset_parameters()\n",
        "        predictor.reset_parameters()\n",
        "        optimizer = torch.optim.Adam(\n",
        "            list(model.parameters()) + list(predictor.parameters()),\n",
        "            lr=args.lr)\n",
        "\n",
        "        for epoch in range(1, 1 + args.epochs):\n",
        "            loss = train(model, predictor, data, split_edge, optimizer,\n",
        "                         args.batch_size)\n",
        "\n",
        "            if epoch % args.eval_steps == 0:\n",
        "                results = test(model, predictor, data, split_edge, evaluator,\n",
        "                               args.batch_size)\n",
        "                for key, result in results.items():\n",
        "                    loggers[key].add_result(run, result)\n",
        "\n",
        "                if epoch % args.log_steps == 0:\n",
        "                    for key, result in results.items():\n",
        "                        train_hits, valid_hits, test_hits = result\n",
        "                        print(key)\n",
        "                        print(f'Run: {run + 1:02d}, '\n",
        "                              f'Epoch: {epoch:02d}, '\n",
        "                              f'Loss: {loss:.4f}, '\n",
        "                              f'Train: {100 * train_hits:.2f}%, '\n",
        "                              f'Valid: {100 * valid_hits:.2f}%, '\n",
        "                              f'Test: {100 * test_hits:.2f}%')\n",
        "                    print('---')\n",
        "\n",
        "        for key in loggers.keys():\n",
        "            print(key)\n",
        "            loggers[key].print_statistics(run)\n",
        "\n",
        "    for key in loggers.keys():\n",
        "        print(key)\n",
        "        loggers[key].print_statistics()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "gE4MG8Sq8j3p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d89f5e38-ac8a-4255-bfcd-49aca908dfd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.5.1+cu121\n",
            "12.1\n",
            "Requirement already satisfied: ogb in /usr/local/lib/python3.10/dist-packages (1.3.6)\n",
            "Requirement already satisfied: torch-geometric in /usr/local/lib/python3.10/dist-packages (2.6.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (2.5.1+cu121)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.26.4)\n",
            "Requirement already satisfied: tqdm>=4.29.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (4.66.6)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.5.2)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (2.2.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.16.0)\n",
            "Requirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (2.2.3)\n",
            "Requirement already satisfied: outdated>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (0.2.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.11.9)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2024.10.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.4)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.2.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.32.3)\n",
            "Requirement already satisfied: setuptools>=44 in /usr/local/lib/python3.10/dist-packages (from outdated>=0.2.0->ogb) (75.1.0)\n",
            "Requirement already satisfied: littleutils in /usr/local/lib/python3.10/dist-packages (from outdated>=0.2.0->ogb) (0.2.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->ogb) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->ogb) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->ogb) (2024.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->ogb) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->ogb) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->ogb) (3.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (3.4.2)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.6.0->ogb) (1.3.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.18.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2024.8.30)\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.5.1+cu121.html\n",
            "Requirement already satisfied: torch-scatter in /usr/local/lib/python3.10/dist-packages (2.1.2+pt25cu121)\n",
            "Requirement already satisfied: torch-sparse in /usr/local/lib/python3.10/dist-packages (0.6.18+pt25cu121)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-sparse) (1.13.1)\n",
            "Requirement already satisfied: numpy<2.3,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from scipy->torch-sparse) (1.26.4)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ogb/linkproppred/dataset_pyg.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  self.data, self.slices = torch.load(self.processed_paths[0])\n",
            "/usr/local/lib/python3.10/dist-packages/ogb/linkproppred/dataset_pyg.py:77: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  train = replace_numpy_with_torchtensor(torch.load(osp.join(path, 'train.pt')))\n",
            "/usr/local/lib/python3.10/dist-packages/ogb/linkproppred/dataset_pyg.py:78: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  valid = replace_numpy_with_torchtensor(torch.load(osp.join(path, 'valid.pt')))\n",
            "/usr/local/lib/python3.10/dist-packages/ogb/linkproppred/dataset_pyg.py:79: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  test = replace_numpy_with_torchtensor(torch.load(osp.join(path, 'test.pt')))\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hits@10\n",
            "Run: 01, Epoch: 10, Loss: 0.1848, Train: 9.67%, Valid: 3.95%, Test: 2.78%\n",
            "Hits@50\n",
            "Run: 01, Epoch: 10, Loss: 0.1848, Train: 29.45%, Valid: 15.03%, Test: 10.02%\n",
            "Hits@100\n",
            "Run: 01, Epoch: 10, Loss: 0.1848, Train: 41.14%, Valid: 22.28%, Test: 16.43%\n",
            "---\n",
            "Hits@10\n",
            "Run: 01, Epoch: 20, Loss: 0.1274, Train: 14.96%, Valid: 6.20%, Test: 3.66%\n",
            "Hits@50\n",
            "Run: 01, Epoch: 20, Loss: 0.1274, Train: 45.18%, Valid: 24.00%, Test: 18.02%\n",
            "Hits@100\n",
            "Run: 01, Epoch: 20, Loss: 0.1274, Train: 60.46%, Valid: 34.60%, Test: 27.52%\n",
            "---\n",
            "Hits@10\n",
            "Run: 01, Epoch: 30, Loss: 0.1023, Train: 22.02%, Valid: 9.43%, Test: 5.51%\n",
            "Hits@50\n",
            "Run: 01, Epoch: 30, Loss: 0.1023, Train: 53.82%, Valid: 27.98%, Test: 21.44%\n",
            "Hits@100\n",
            "Run: 01, Epoch: 30, Loss: 0.1023, Train: 69.82%, Valid: 39.19%, Test: 31.59%\n",
            "---\n",
            "Hits@10\n",
            "Run: 01, Epoch: 40, Loss: 0.0880, Train: 27.68%, Valid: 12.53%, Test: 8.11%\n",
            "Hits@50\n",
            "Run: 01, Epoch: 40, Loss: 0.0880, Train: 55.99%, Valid: 30.86%, Test: 23.92%\n",
            "Hits@100\n",
            "Run: 01, Epoch: 40, Loss: 0.0880, Train: 70.76%, Valid: 41.48%, Test: 33.92%\n",
            "---\n",
            "Hits@10\n",
            "Run: 01, Epoch: 50, Loss: 0.0748, Train: 27.67%, Valid: 11.17%, Test: 6.76%\n",
            "Hits@50\n",
            "Run: 01, Epoch: 50, Loss: 0.0748, Train: 63.51%, Valid: 33.49%, Test: 25.91%\n",
            "Hits@100\n",
            "Run: 01, Epoch: 50, Loss: 0.0748, Train: 76.14%, Valid: 43.50%, Test: 35.26%\n",
            "---\n",
            "Hits@10\n",
            "Run: 01, Epoch: 60, Loss: 0.0660, Train: 37.01%, Valid: 16.10%, Test: 11.47%\n",
            "Hits@50\n",
            "Run: 01, Epoch: 60, Loss: 0.0660, Train: 73.11%, Valid: 40.37%, Test: 32.25%\n",
            "Hits@100\n",
            "Run: 01, Epoch: 60, Loss: 0.0660, Train: 85.36%, Valid: 51.40%, Test: 42.71%\n",
            "---\n",
            "Hits@10\n",
            "Run: 01, Epoch: 70, Loss: 0.0607, Train: 46.08%, Valid: 22.93%, Test: 15.92%\n",
            "Hits@50\n",
            "Run: 01, Epoch: 70, Loss: 0.0607, Train: 73.31%, Valid: 41.23%, Test: 32.69%\n",
            "Hits@100\n",
            "Run: 01, Epoch: 70, Loss: 0.0607, Train: 86.20%, Valid: 52.30%, Test: 43.34%\n",
            "---\n",
            "Hits@10\n",
            "Run: 01, Epoch: 80, Loss: 0.0555, Train: 53.18%, Valid: 26.98%, Test: 19.85%\n",
            "Hits@50\n",
            "Run: 01, Epoch: 80, Loss: 0.0555, Train: 79.17%, Valid: 45.88%, Test: 37.19%\n",
            "Hits@100\n",
            "Run: 01, Epoch: 80, Loss: 0.0555, Train: 88.53%, Valid: 54.30%, Test: 45.39%\n",
            "---\n",
            "Hits@10\n",
            "Run: 01, Epoch: 90, Loss: 0.0514, Train: 58.34%, Valid: 32.15%, Test: 23.94%\n",
            "Hits@50\n",
            "Run: 01, Epoch: 90, Loss: 0.0514, Train: 81.72%, Valid: 49.09%, Test: 39.65%\n",
            "Hits@100\n",
            "Run: 01, Epoch: 90, Loss: 0.0514, Train: 89.57%, Valid: 55.98%, Test: 46.60%\n",
            "---\n",
            "Hits@10\n",
            "Run: 01, Epoch: 100, Loss: 0.0472, Train: 55.11%, Valid: 27.82%, Test: 20.59%\n",
            "Hits@50\n",
            "Run: 01, Epoch: 100, Loss: 0.0472, Train: 83.64%, Valid: 48.96%, Test: 39.72%\n",
            "Hits@100\n",
            "Run: 01, Epoch: 100, Loss: 0.0472, Train: 91.14%, Valid: 56.36%, Test: 47.02%\n",
            "---\n",
            "Hits@10\n",
            "Run: 01, Epoch: 110, Loss: 0.0439, Train: 49.65%, Valid: 23.86%, Test: 17.34%\n",
            "Hits@50\n",
            "Run: 01, Epoch: 110, Loss: 0.0439, Train: 81.42%, Valid: 45.94%, Test: 36.80%\n",
            "Hits@100\n",
            "Run: 01, Epoch: 110, Loss: 0.0439, Train: 91.01%, Valid: 55.27%, Test: 45.94%\n",
            "---\n",
            "Hits@10\n",
            "Run: 01, Epoch: 120, Loss: 0.0418, Train: 45.91%, Valid: 20.85%, Test: 14.25%\n",
            "Hits@50\n",
            "Run: 01, Epoch: 120, Loss: 0.0418, Train: 85.22%, Valid: 49.24%, Test: 40.06%\n",
            "Hits@100\n",
            "Run: 01, Epoch: 120, Loss: 0.0418, Train: 90.82%, Valid: 54.81%, Test: 45.35%\n",
            "---\n",
            "Hits@10\n",
            "Run: 01, Epoch: 130, Loss: 0.0394, Train: 51.24%, Valid: 22.99%, Test: 16.58%\n",
            "Hits@50\n",
            "Run: 01, Epoch: 130, Loss: 0.0394, Train: 84.82%, Valid: 48.10%, Test: 38.90%\n",
            "Hits@100\n",
            "Run: 01, Epoch: 130, Loss: 0.0394, Train: 92.54%, Valid: 56.12%, Test: 46.79%\n",
            "---\n",
            "Hits@10\n",
            "Run: 01, Epoch: 140, Loss: 0.0370, Train: 58.42%, Valid: 30.72%, Test: 22.66%\n",
            "Hits@50\n",
            "Run: 01, Epoch: 140, Loss: 0.0370, Train: 87.23%, Valid: 51.84%, Test: 42.57%\n",
            "Hits@100\n",
            "Run: 01, Epoch: 140, Loss: 0.0370, Train: 93.29%, Valid: 58.38%, Test: 49.13%\n",
            "---\n",
            "Hits@10\n",
            "Run: 01, Epoch: 150, Loss: 0.0357, Train: 54.11%, Valid: 26.92%, Test: 20.56%\n",
            "Hits@50\n",
            "Run: 01, Epoch: 150, Loss: 0.0357, Train: 88.36%, Valid: 52.37%, Test: 43.39%\n",
            "Hits@100\n",
            "Run: 01, Epoch: 150, Loss: 0.0357, Train: 93.30%, Valid: 57.91%, Test: 48.95%\n",
            "---\n",
            "Hits@10\n",
            "Run: 01, Epoch: 160, Loss: 0.0338, Train: 51.67%, Valid: 24.82%, Test: 18.20%\n",
            "Hits@50\n",
            "Run: 01, Epoch: 160, Loss: 0.0338, Train: 87.31%, Valid: 50.60%, Test: 41.32%\n",
            "Hits@100\n",
            "Run: 01, Epoch: 160, Loss: 0.0338, Train: 94.19%, Valid: 58.62%, Test: 49.37%\n",
            "---\n",
            "Hits@10\n",
            "Run: 01, Epoch: 170, Loss: 0.0323, Train: 52.58%, Valid: 22.86%, Test: 16.48%\n",
            "Hits@50\n",
            "Run: 01, Epoch: 170, Loss: 0.0323, Train: 89.13%, Valid: 51.22%, Test: 41.60%\n",
            "Hits@100\n",
            "Run: 01, Epoch: 170, Loss: 0.0323, Train: 94.27%, Valid: 57.58%, Test: 48.07%\n",
            "---\n",
            "Hits@10\n",
            "Run: 01, Epoch: 180, Loss: 0.0306, Train: 46.97%, Valid: 21.04%, Test: 15.57%\n",
            "Hits@50\n",
            "Run: 01, Epoch: 180, Loss: 0.0306, Train: 89.00%, Valid: 51.72%, Test: 42.52%\n",
            "Hits@100\n",
            "Run: 01, Epoch: 180, Loss: 0.0306, Train: 95.56%, Valid: 59.85%, Test: 50.57%\n",
            "---\n",
            "Hits@10\n",
            "Run: 01, Epoch: 190, Loss: 0.0300, Train: 66.88%, Valid: 32.51%, Test: 25.37%\n",
            "Hits@50\n",
            "Run: 01, Epoch: 190, Loss: 0.0300, Train: 89.82%, Valid: 51.56%, Test: 42.52%\n",
            "Hits@100\n",
            "Run: 01, Epoch: 190, Loss: 0.0300, Train: 95.58%, Valid: 59.32%, Test: 50.17%\n",
            "---\n",
            "Hits@10\n",
            "Run: 01, Epoch: 200, Loss: 0.0282, Train: 49.53%, Valid: 23.32%, Test: 16.73%\n",
            "Hits@50\n",
            "Run: 01, Epoch: 200, Loss: 0.0282, Train: 91.37%, Valid: 54.25%, Test: 44.77%\n",
            "Hits@100\n",
            "Run: 01, Epoch: 200, Loss: 0.0282, Train: 95.96%, Valid: 60.54%, Test: 51.32%\n",
            "---\n",
            "Hits@10\n",
            "Run: 01, Epoch: 210, Loss: 0.0267, Train: 52.30%, Valid: 23.17%, Test: 18.66%\n",
            "Hits@50\n",
            "Run: 01, Epoch: 210, Loss: 0.0267, Train: 89.16%, Valid: 51.00%, Test: 41.65%\n",
            "Hits@100\n",
            "Run: 01, Epoch: 210, Loss: 0.0267, Train: 96.19%, Valid: 60.14%, Test: 50.56%\n",
            "---\n",
            "Hits@10\n",
            "Run: 01, Epoch: 220, Loss: 0.0271, Train: 41.39%, Valid: 17.51%, Test: 12.46%\n",
            "Hits@50\n",
            "Run: 01, Epoch: 220, Loss: 0.0271, Train: 91.08%, Valid: 53.20%, Test: 43.62%\n",
            "Hits@100\n",
            "Run: 01, Epoch: 220, Loss: 0.0271, Train: 96.49%, Valid: 60.90%, Test: 51.53%\n",
            "---\n",
            "Hits@10\n",
            "Run: 01, Epoch: 230, Loss: 0.0261, Train: 49.14%, Valid: 20.66%, Test: 16.50%\n",
            "Hits@50\n",
            "Run: 01, Epoch: 230, Loss: 0.0261, Train: 90.77%, Valid: 51.82%, Test: 42.16%\n",
            "Hits@100\n",
            "Run: 01, Epoch: 230, Loss: 0.0261, Train: 96.19%, Valid: 59.51%, Test: 50.01%\n",
            "---\n",
            "Hits@10\n",
            "Run: 01, Epoch: 240, Loss: 0.0253, Train: 43.91%, Valid: 20.85%, Test: 14.36%\n",
            "Hits@50\n",
            "Run: 01, Epoch: 240, Loss: 0.0253, Train: 92.07%, Valid: 54.05%, Test: 44.20%\n",
            "Hits@100\n",
            "Run: 01, Epoch: 240, Loss: 0.0253, Train: 96.95%, Valid: 61.06%, Test: 51.61%\n",
            "---\n",
            "Hits@10\n",
            "Run: 01, Epoch: 250, Loss: 0.0243, Train: 55.42%, Valid: 23.70%, Test: 18.92%\n",
            "Hits@50\n",
            "Run: 01, Epoch: 250, Loss: 0.0243, Train: 93.27%, Valid: 54.63%, Test: 44.86%\n",
            "Hits@100\n",
            "Run: 01, Epoch: 250, Loss: 0.0243, Train: 97.21%, Valid: 60.79%, Test: 51.24%\n",
            "---\n",
            "Hits@10\n",
            "Run: 01, Epoch: 260, Loss: 0.0235, Train: 56.90%, Valid: 27.26%, Test: 20.14%\n",
            "Hits@50\n",
            "Run: 01, Epoch: 260, Loss: 0.0235, Train: 91.04%, Valid: 52.45%, Test: 42.66%\n",
            "Hits@100\n",
            "Run: 01, Epoch: 260, Loss: 0.0235, Train: 96.33%, Valid: 59.50%, Test: 49.71%\n",
            "---\n"
          ]
        }
      ]
    }
  ]
}